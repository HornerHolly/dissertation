---
layout: post
title:  "Exercise 6"
date:   2021-02-26 12:21:16 -0500
categories: jekyll update
---
# Work Log

    - Friday, 2/12 (11:20-12:00pm; 5:00pm-6:00pm): attended lab hours; cleaned up R code for word frequency calculations.
    - Monday, 2/15 (8:40-11:30am): experimented with Open Refine and Breve; installed RStudio packages; attended workshop on Github Desktop.

# Exercise Description

I restructured my tabuluar data to consider the frequencies of types of words in a single poem, rather than relying on my metadata for this exercise. To do this, I used my pre-existing RStudio code to tabulate word frequencies for all of Smith's poetry (Note: this code still needs some modification). This created a master word frequency list and one for each poem. For this excercise, I looked at "Beachy Head" the poem (not the entire volume) and sorted the word frequencies to reveal the first and last 20 words with the highest and lowest frequency. I then classified these words as either (i.e. noun, verb, adverb, conjunction, determiner etc) based on their part of speech. The full list for this can be found under the Exercise6_Parts csv on the main page.

I then ran my tabular data through Breve and played around with Google Data Studio. Here is the link to my Google Data Studio experiment: https://datastudio.google.com/s/l20RMQ8if_A
RStudio Code

For the sake of backing up my data, I am including my current RStudio code for word frequencies here:

# Test


```
rm(list=ls()) library(tm) library(SnowballC) setwd("E:/R Codes for School/Dissertation/Smith/PlainText_Smith/SmithSoundALL")
LOOP TO ITERATE THROUGH ALL OF THE POEMS
SETTING UP THE LOOP

path = "E:/R Codes for School/Dissertation/Smith/PlainText_Smith/SmithSoundALL" file.names <- dir(path, pattern =NULL) AA_Master <- setNames(data.frame(matrix(ncol = 2, nrow = 0)), c("V1", "V2"))
```

# FOR LOOP


```
for(i in 1:length(file.names)){ file<-scan(file.names[i], "character", sep="\n") file<- stemDocument(file) file<-tolower(file) file<-gsub("[[:punct:]]","",file) file<-gsub("Ã¢???T","",file) #Split sentence words<-strsplit(file," ") words <- unlist(words)
```

# Calculate word frequencies


```
words.freq<-table(words) file<-cbind(names(words.freq),as.integer(words.freq))
drop file name

file<-file[-1,] AA_Master <- merge(x = AA_Master, y = file, by = "V1", all = TRUE) assign(file.names[i], file) }
```

# Cleaning Merged Data


```
AAAMaster <- AA_Master[,-1] AAAMaster[] <- lapply(AAAMaster, as.numeric) AAAMaster[is.na(AAAMaster)] <- 0 AA_Master$"Total" <- rowSums(AAAMaster[1:133], na.rm=TRUE) AAA_ALL<- AA_Master[-c(2:134)] colnames(AAA_ALL)<-c("Word", "Frequency")'

remove(AA_Master) remove(AAAMaster)
```

# stopwords


```
b<-1 for(b in 1:length(AAA_ALL)){ AAA_ALL2 <- data.frame(AAA_ALL[!grepl(stopwords("en")[b],AAA_ALL[,1]),]) }
```

# Calculate word frequencies


```
words.freq2<-table(AAA_ALL2) file<-cbind(names(words.freq2),as.integer(words.freq2))
```

# Save progress


```
write.csv(AAA_ALL,'AAA_ALL.csv') 
```
